apiVersion: spark.apache.org/v1beta1
kind: SparkApplication
metadata:
  name: spark-connect-server
spec:
  mainClass: "org.apache.spark.sql.connect.service.SparkConnectServer"
  jars: "local://delta-spark_2.13-4.0.0.jar"
  sparkConf:
    spark.master: "k8s://https://lb-apiserver-dev.kubernetes-dev.local:8443" # Run kubectl cluster-info to get info
    spark.deployMode: "cluster"
    spark.executor.cores: "1"
    spark.cores.max: "3"
    spark.kubernetes.authenticate.driver.serviceAccountName: "spark"
    spark.kubernetes.container.image: "apache/spark:4.0.0"
    spark.jars.packages: "io.delta:delta-spark_2.13:4.0.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.1"
    spark.ui.reverseProxy: "true"
    spark.sql.connect.grpc.binding: "0.0.0.0:15002"
    spark.driver.extraJavaOptions: "-Dhttp.proxyHost=<> -Dhttp.proxyPort=<> -Dhttps.proxyHost=<> -Dhttps.proxyPort=<> -Divy.cache.dir=/tmp -Divy.home=/tmp"
    spark.jars.ivy: "/opt/spark/.ivy2"
    spark.sql.defaultCatalog: "spark_catalog"
    # config hdfs
    spark.hadoop.fs.defaultFS: "hdfs://hdfs-cluster.lakehouse-dev.cads.live:8020"
    spark.sql.catalogImplementation: "hive"
    spark.hadoop.hive.metastore.uris: "thrift://172.24.177.28:9083"
    # config delta
    spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
    spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
    # config s3
    spark.hadoop.fs.s3a.endpoint: "https://s3.amazonaws.com"
    spark.hadoop.fs.s3a.access.key: "YOUR_ACCESS_KEY"
    spark.hadoop.fs.s3a.secret.key: "YOUR_SECRET_KEY"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.path.style.access: "true"
    # config iceberg
    spark.sql.catalog.my_catalog: "org.apache.iceberg.spark.SparkCatalog"
    spark.sql.catalog.my_catalog.type: "hadoop"
    spark.sql.catalog.my_catalog.warehouse: "s3a://my-bucket/iceberg/"
  runtimeVersions:
    sparkVersion: "4.0.0"
  driverSpec:
    podTemplateSpec:
      spec:
        containers:
         - ports:
            - protocol: TCP
              containerPort: 15002
        hostAliases:
          - ip: "172.24.177.51"
            hostnames:
              - "binhln.cads.live"
  driverServiceIngressList:
    - serviceMetadata:
        name: "spark-connect-service"
      serviceSpec:
        ports:
          - protocol: TCP
            port: 15002
            targetPort: 15002
      ingressMetadata:
        name: "spark-connect-ingress"
        annotations:
          kubernetes.io/ingress.class: nginx
          nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
---
apiVersion: v1
kind: Service
metadata:
  name: spark-connect-service
  namespace: lakehouse
spec:
  type: NodePort
  selector:
    spark-role: driver
    spark-app-name: spark-connect-server
  ports:
    - name: connect-port
      port: 15002
      targetPort: 15002
      nodePort: 30000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  name: my-ingress
spec:
  rules:
    - host: binhln.cads.live
      http:
        paths:
          - backend:
              service:
                name: spark-connect-service
                port:
                  number: 15002
            path: /
            pathType: ImplementationSpecific